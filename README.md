# ğŸ“¡ MonitorizaciÃ³n de Red con Prometheus y Flask

Este proyecto tiene como objetivo desplegar una soluciÃ³n ligera de monitoreo de red usando Python, Flask y Prometheus. La aplicaciÃ³n permite realizar pings a mÃºltiples dispositivos definidos por IP y expone mÃ©tricas de latencia ICMP que pueden ser recolectadas por Prometheus.

---

## ğŸš€ CaracterÃ­sticas Principales

- ğŸ“¦ App escrita en Flask, con mÃ©tricas expuestas vÃ­a `/metrics`
- ğŸ“ˆ RecolecciÃ³n de mÃ©tricas ICMP por IP usando `ping3`
- ğŸ§  IntegraciÃ³n con Prometheus vÃ­a `ServiceMonitor` en Kubernetes
- ğŸ³ Dockerfile optimizado para producciÃ³n con Gunicorn
- â˜¸ï¸ Manifiestos de Kubernetes para despliegue, servicio y monitoreo
- ğŸ”§ ConfiguraciÃ³n modular con variables de entorno para IPs objetivo

---

## ğŸ§¬ Estructura del proyecto




## ğŸ“˜ MonitorizaciÃ³n con Prometheus en Kubernetes
Este repositorio contiene la configuraciÃ³n personalizada para desplegar Prometheus utilizando Helm en un clÃºster Kubernetes, ideal para entornos de laboratorio y pruebas.

## ğŸ”§ Componentes Instalados
- Prometheus Server
- Alertmanager
- Node Exporter
- PushGateway
- Kube-State-Metrics

## ğŸ¯ Objetivos
- Recolectar mÃ©tricas de los nodos y aplicaciones.
- Visualizar mÃ©tricas vÃ­a Prometheus UI.
- Preparar el entorno para conectar con Grafana.
- Optimizar el despliegue para entornos con recursos limitados.
âš™ï¸ Instalacion:
cd /infra/prometheus #ahi se encuentra el custom value.yaml
helm install prometheus prometheus-community/prometheus \
  -n monitoring \
  -f custom-values.yaml
Nota: La persistencia estÃ¡ desactivada para facilitar la instalaciÃ³n sin volÃºmenes en nodos sin StorageClass.

## ğŸ“ Estructura del repositorio

ping-monitor/
â”œâ”€â”€ Infra/Prometheus/ custom-values.yaml         # ConfiguraciÃ³n para Helm
â”œâ”€â”€ manifests/                 # Archivos YAML opcionales
â”œâ”€â”€ README.md                  # DocumentaciÃ³n del proyecto
â””â”€â”€ .gitignore                 # Archivos excluidos del repo


## ğŸš¨ Advertencia
Este despliegue no persiste datos. Todos los datos se borran si el pod se reinicia. Ideal para pruebas, no recomendado para producciÃ³n.

Cuando lo tengas en Git, podÃ©s agregar etiquetas y documentar tus pruebas, alertas, y dashboards. Â¿QuerÃ©s que despuÃ©s te ayude a crear un panel en Grafana para que se integre directo con este setup? Lo armamos paso a paso ğŸ’¡ğŸ“Š


---

## ğŸ›  CÃ³mo levantar el sistema

```bash
# Instalar Prometheus con Helm y configuraciÃ³n personalizada
helm install prometheus prometheus-community/prometheus \
  -n monitoring \
  -f custom-values.yaml


## ğŸ’¡ Notas
- La app lee las IPs desde la variable de entorno TARGET_IPS, separadas por comas.
- Si no se define la variable, se usan IPs por defecto para prueba.
- Los pings fallidos se marcan con latencia -1.


ğŸ“˜ network-monitor Integration with Prometheus

ğŸ—“ï¸ Fecha: 6 de agosto, 2025  
ğŸ§‘â€ğŸ’» Autor: Steven

âœ… Objetivo
Integrar el servicio network-monitor con Prometheus para recolectar mÃ©tricas de latencia ICMP hacia dispositivos de red.

ğŸ“¦ Componentes involucrados
- Kubernetes
- Prometheus Operator (kube-prometheus-stack)
- ServiceMonitor
- Grafana (opcional para visualizaciÃ³n)
- Flask + prometheus_client (en el servicio)

ğŸ”§ Pasos realizados

1. VerificaciÃ³n del endpoint de mÃ©tricas

curl http://network-monitor-service.monitoring.svc.cluster.local:8080/metrics

Se confirmÃ³ que el endpoint responde correctamente con mÃ©tricas como:
- device_ping_latency_ms{ip="..."} â†’ Latencia ICMP
- MÃ©tricas estÃ¡ndar de Python y del proceso

2. CorrecciÃ³n del ServiceMonitor

Se detectÃ³ que Prometheus no estaba scrapeando el servicio debido a un label incorrecto.

SoluciÃ³n aplicada:
- Se editÃ³ el recurso activo:
  kubectl edit servicemonitor network-monitor-servicemonitor -n monitoring

- Se actualizÃ³ el archivo YAML:
  metadata:
    labels:
      release: kube-prometheus-stack

Esto asegura compatibilidad con el selector de serviceMonitorSelector usado por Prometheus.

3. VerificaciÃ³n en Prometheus

- Se accediÃ³ a la UI de Prometheus
- En Status â†’ Targets, se confirmÃ³ que el job network-monitor-servicemonitor aparece como UP

ğŸ“Œ PrÃ³ximos pasos
- [ ] Crear panel en Grafana para visualizar device_ping_latency_ms
- [ ] Configurar alertas para valores -1.0 o latencias elevadas
- [ ] Expandir monitoreo a mÃ¡s dispositivos o protocolos

ğŸ“ Notas
Este README documenta la integraciÃ³n exitosa del servicio de monitoreo de red con Prometheus. El sistema ahora puede recolectar mÃ©tricas de latencia ICMP y estÃ¡ listo para visualizaciÃ³n y alertas.



GRAFANA ACCESS GUIDE - kube-prometheus-stack
============================================

This document explains how to access Grafana deployed via the kube-prometheus-stack Helm chart,
including port-forwarding, credential retrieval, and troubleshooting common errors.

------------------------------------------------------------
1. PORT-FORWARDING TO ACCESS GRAFANA
------------------------------------------------------------

Grafana runs internally on port 3000, but the Kubernetes Service exposes port 80.

To forward it to your local machine:

    kubectl port-forward svc/kube-prometheus-stack-grafana 3001:80 -n monitoring

NOTE:
- If port 3001 is already in use, choose another (e.g. 3002, 8080, etc.)

------------------------------------------------------------
2. RETRIEVE GRAFANA CREDENTIALS
------------------------------------------------------------

Grafana credentials are stored in a Kubernetes Secret.

To retrieve them:

    # Username
    kubectl get secret kube-prometheus-stack-grafana -n monitoring \
      -o jsonpath="{.data.admin-user}" | base64 --decode

    # Password
    kubectl get secret kube-prometheus-stack-grafana -n monitoring \
      -o jsonpath="{.data.admin-password}" | base64 --decode

In this environment:
- Username: admin
- Password: prom-operator

------------------------------------------------------------
3. COMMON ERRORS & FIXES
------------------------------------------------------------

âŒ ERROR: "Service does not have a service port 3000"

FIX:
Use the correct exposed port (80), not Grafana's internal 3000.

    kubectl port-forward svc/kube-prometheus-stack-grafana 3001:80 -n monitoring

âŒ ERROR: "address already in use"

FIX:
- Check which process is using the port:

     

