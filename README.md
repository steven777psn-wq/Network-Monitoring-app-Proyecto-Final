![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
![CI/CD: GitHub Actions](https://img.shields.io/github/actions/workflow/status/steven777psn-wq/Network-Monitoring-app-Proyecto-Final/ci.yaml?label=CI%2FCD)
![Dockerized](https://img.shields.io/badge/Docker-ready-blue)

# ğŸ›°ï¸ Network Monitoring Project

This project provides a lightweight, containerized network monitoring solution using Python, Prometheus, Alertmanager, and Grafana dashboards. It is designed for educational and practical use, with a focus on clarity, reproducibility, and modular deployment.

## ğŸ“š Table of Contents

- [Features](#features)
- [Technologies Used](#technologies-used)
- [Metrics Overview](#metrics-overview)
- [Deployment](#deployment)
- [Alerting System](#alerting-system)
- [Lab Integration (EVE-NG)](#lab-integration-eve-ng)
- [Automation Components](#automation-components)
- [Screenshots](#process-evidence)
- [Contributions](#contributions)
- [License](#license)


## ğŸ“¦ Project Structure
```
ping-monitor/
â”œâ”€â”€ ansible/
â”‚   â””â”€â”€ playbooks/
â”‚       â”œâ”€â”€ deploy-monitoring.yml         # Deploys Prometheus, Grafana, Alertmanager, Telegram webhook
â”‚       â”œâ”€â”€ validate.yml                  # Validates service health via internal cluster endpoints
â”‚       â””â”€â”€ cleanup-all-namespaces.yml    # Cleans up completed/failed pods across key namespaces
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yaml                       # Full CI/CD pipeline: cleanup, deploy, validate
â”‚       â””â”€â”€ deploy.yaml                   # Lightweight deploy-only workflow
â”œâ”€â”€ infra/
â”‚   â”œâ”€â”€ prometheus/                       # Prometheus config, rules, and values
â”‚   â””â”€â”€ alertmanager/                     # Alertmanager configuration
â”œâ”€â”€ k8s/
â”‚   â”œâ”€â”€ permisos-rbac/                    # Prometheus RBAC manifests
â”‚   â”œâ”€â”€ monitoring/                       # Monitoring service manifests
â”‚   â”œâ”€â”€ deployments/                      # Network monitor deployment and ServiceMonitor
â”‚   â””â”€â”€ telegram-webhook/                 # Telegram webhook deployment, secret, and service
â”œâ”€â”€ dashboards/
â”‚   â””â”€â”€ ping-latency-dashboard.json       # Grafana dashboard for latency visualization
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/                              # Core app logic (ping utils, config, main)
â”‚   â””â”€â”€ telegram-webhook/                 # Telegram webhook service (Dockerized)
â”œâ”€â”€ scripts/                              # Setup and cleanup scripts
â”œâ”€â”€ .env                                  # Environment variables
â”œâ”€â”€ check_env.py                          # Environment validation script
â”œâ”€â”€ docker-compose.yaml                   # Local orchestration
â”œâ”€â”€ Dockerfile                            # Container build for app or webhook
â”œâ”€â”€ requirements.txt                      # Python dependencies
â”œâ”€â”€ README.md                             # Project documentation
```
---

## ğŸš€ Features

- ICMP-based latency monitoring using `ping3`
- Prometheus metrics endpoint (`/metrics`)
- Status endpoint (`/status`) for basic health checks
- Custom metric: `device_ping_latency_ms`
- Alertmanager integration with sample alert rules
- Telegram webhook integration for real-time notifications
- Alert routing based on Prometheus job labels
- Docker Compose and Kubernetes support
- Helm-based Prometheus stack deployment
- Grafana integration

## ğŸ§° Technologies Used

| Technology         | Purpose                                  | Notes / Integration                      |
|--------------------|-------------------------------------------|------------------------------------------|
| Python + ping3     | ICMP-based latency monitoring             | Custom metrics exposed via `/metrics`    |
| Prometheus         | Metrics scraping and alert rule engine    | Deployed via Helm                        |
| Grafana            | Dashboard visualization                   | Latency, outage heatmaps                 |
| Alertmanager       | Alert routing                             | Integrated with Telegram webhook         |
| Telegram Webhook   | Real-time notifications                   | Custom Flask app                         |
| Docker Compose     | Local orchestration                       | Multi-container setup                    |
| Kubernetes         | Production-grade deployment               | Manifests + Helm charts                  |
| Ansible            | Automation of deployment and validation   | Playbooks for CI/CD                      |
| GitHub Actions     | CI/CD pipeline                            | Cleanup, deploy, validate workflows      |
| EVE-NG             | Lab simulation environment                | Virtual routers, firewalls, hosts        |

---

## ğŸ§ª Metrics Overview

The Python app pings a list of devices and exposes latency metrics:

Example text:
device_ping_latency_ms{device="123.1.1.1"} 23.5
device_ping_latency_ms{device="123.1.1.1"} -1

â€¢ 	-1 indicates unreachable or failed ping.
â€¢       0 also considered unreachable in alert logic
â€¢ 	Metrics are refreshed every 30 seconds.

ğŸ”§ Local Deployment (Docker Compose)

cd infra/
docker-compose up --build

â˜ï¸ Kubernetes Deploymen
1. Apply manifests

kubectl apply -f k8s/YAMLs

Includes:
â€¢ 	Deployment and Service for the Python app
â€¢ 	Prometheus  for scraping metrics
â€¢ 	Alertmanager configuration
â€¢       Telegram webhook deployment and service

## ğŸ“Š Monitored Metrics

| Metric Name                  | Description                          | PromQL Example                          |
|------------------------------|--------------------------------------|-----------------------------------------|
| `device_ping_latency_ms`     | Latency per device (ICMP)            | `avg(device_ping_latency_ms)`           |
| `up`                         | Service availability                 | `up == 0` for unreachable targets       |
| `probe_success` (optional)   | Ping success indicator               | `sum(probe_success)`                    |

2. Deploy Prometheus stack via Helm

helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack -f infra/prometheus/custom-values.yaml -n monitoring

Note: Ensure your app's Service has correct labels for Prometheus discovery.

3. Access Grafana

kubectl port-forward svc/kube-prometheus-stack-grafana 3001:80 -n monitoring
kubectl get secret kube-prometheus-stack-grafana -n monitoring -o jsonpath="{.data.admin-password}" | base64 -d

ğŸ“ˆ Planned Grafana Dashboard
â€¢       Latency over time per device
â€¢       Alert visualization for unreachable hosts
â€¢       Outage heatmap per zone


ğŸš¨ Alerting System

â€¢       Prometheus alert rules defined in infra/prometheus/prometheus-rule.yaml
â€¢       Alerts include:
        - High latency detection
        - Outage detection (<= 0 latency)
â€¢       Alertmanager routes only alerts from job="network-monitor-service" to Telegram
â€¢       Telegram webhook receives alerts via HTTP and forwards them to your bot


ğŸ“¬ Telegram Webhook Integration
â€¢ 	Custom Flask app deployed as a Kubernetes service
â€¢ 	Receives alerts from Alertmanager via webhook
â€¢ 	Filters and formats messages for Telegram delivery
â€¢ 	Alertmanager config uses matchers to route only relevant alerts
        - matchers:
          - job = "network-monitor-service"

ğŸ§ª EVE-NG Lab Integration

 This project is also deployed and tested within a custom EVE-NG lab environment, allowing for realistic network simulations scenarios.
â€¢ 	The lab includes virtual routers, switches, Win Server, Palo Alto Firewall and Linux & Windows hosts configured to respond to ICMP probes.
â€¢ 	Prometheus and the Python monitoring app are deployed in isolated containers within the lab.
â€¢ 	This setup enables controlled testing of latency, packet loss, and alerting behavior under various network conditions.
â€¢ 	EVE-NG provides a visual topology and supports reproducible demos for testing and education.

ğŸ” Monitoring Integration
â€¢ 	The Python monitoring app pings key devices across VLANs
â€¢ 	Prometheus scrapes metrics from the app, enabling visibility into latency and reachability across zones
â€¢ 	Alertmanager triggers notifications when devices become unreachable or latency exceeds thresholds
â€¢ 	Telegram webhook delivers alerts in real time to your configured bot/channe

ğŸ“¦ Lab Monitoring Stack: CI/CD + Ansible Automation

This repository automates the deployment and validation of a Kubernetes-based monitoring stack using Ansible and GitHub Actions. It includes Prometheus, Grafana, Alertmanager, and a Telegram webhook, with full CI/CD integration and healthcheck routines.

ğŸ§° Components
- Ansible Playbooks for deployment, cleanup, and validation
- GitHub Actions Workflows for CI/CD automation
- Kubernetes Manifests for RBAC, services, and monitoring pods

## âš™ï¸ Automation Components

| Component             | Description                                  | Location / File                         |
|-----------------------|----------------------------------------------|------------------------------------------|
| Deployment Playbook   | Applies all Kubernetes manifests             | `ansible/playbooks/deploy-monitoring.yml` |
| Validation Playbook   | Healthchecks for Prometheus, Grafana, webhook| `ansible/playbooks/validate.yml`         |
| Cleanup Playbook      | Deletes residual pods across namespaces      | `ansible/playbooks/cleanup-all-namespaces.yml` |
| CI/CD Workflow        | Full pipeline: cleanup, deploy, validate     | `.github/workflows/ci.yaml`              |
| Lightweight Deploy    | Deploy-only workflow                         | `.github/workflows/deploy.yaml`          |

ğŸ“ Directory Structure:
```
ğŸ“ ping-monitor/
â”œâ”€â”€ ansible/
â”‚   â””â”€â”€ playbooks/
â”‚       â”œâ”€â”€ deploy-monitoring.yml
â”‚       â”œâ”€â”€ validate.yml
â”‚       â””â”€â”€ cleanup-all-namespaces.yml
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yaml
â”‚       â””â”€â”€ deploy.yaml
```
ğŸš€ Deployment Playbook: deploy-monitoring.yml
Applies all Kubernetes manifests required for the monitoring stack:
- Prometheus RBAC, rules, and config
- Alertmanager configuration
- Telegram webhook
- Network monitoring services

# ansible-playbook ansible/playbooks/deploy-monitoring.yml

âœ… Validation Playbook: validate.yml
Performs healthchecks using internal cluster DNS:
- Prometheus readiness (/-/ready)
- Grafana availability
- Telegram webhook pod status
Includes conditional notifications for success or failure

# ansible-playbook ansible/playbooks/validate.yml

ğŸ§¹ Cleanup Playbook: cleanup-all-namespaces.yml
Deletes residual pods across key namespaces:
- Completed pods
- Failed pods
- Pods with ContainerStatusUnknown
ansible-playbook ansible/playbooks/cleanup-all-namespaces.yml

# ansible-playbook ansible/playbooks/cleanup-all-namespaces.yml

âš™ï¸ GitHub Actions Workflows
ci.yaml: Full CI/CD Pipeline
Runs on every push to main or manual trigger. Steps:
- Checkout repo
- Install dependencies (ansible, kubectl)
- Cleanup residual pods
- Deploy monitoring stack
- Validate services

name: RootZone CI/CD Pipeline
on:
  push:
    branches: [ "main" ]
  workflow_dispatch:
  
deploy.yaml: Lightweight Deployment
Runs only the deployment playbook on push to main.

name: Deploy Monitoring Stack
on:
  push:
    branches:
      - main

Both workflows run on a self-hosted runner for full control over the environment

ğŸ§ª Pre-commit Validation
Before pushing changes:

# Lint for best practices
ansible-lint ansible/playbooks/*.yml

# Dry-run to preview changes
ansible-playbook ansible/playbooks/deploy-monitoring.yml --check

ğŸ“¬ Notifications & Healthchecks
The validation playbook includes logic to:
- Show endpoint status
- Detect failures
- Print success or failure messages
It can be extended to send Telegram alerts...(Will work on this latter) 

ğŸ§  Notes
- File paths are dynamically resolved.
- Error handling is built-in with ignore_errors: true and conditional blocks.

## Contributions

This project is open to improvements. Please refer to the [CONTRIBUTING.md](CONTRIBUTING.md) guide to learn how to collaborate.

## License

This project is licensed under the MIT License. See [LICENSE](LICENSE) for more details.

## Process Evidence

Screenshots of deployment, monitoring, and alerting are available in the `/screenshots` folder.


End...


